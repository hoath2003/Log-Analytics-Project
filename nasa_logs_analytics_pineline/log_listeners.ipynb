{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c35fdb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_extract, split, col, when, unix_timestamp, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c156cf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Configuration for Cassandra and Kafka\n",
    "cassandra_host = \"cassandra\"\n",
    "cassandra_user = \"cassandra\"\n",
    "cassandra_pwd  = \"cassandra\"\n",
    "cassandra_port = 9042\n",
    "key_space = \"loganalysis\"\n",
    "table_name = \"nasalog\"\n",
    "kafka_server = \"kafka:9092\"\n",
    "kafka_topic = \"nasa_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf24a14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create SparkSession with Kafka and Cassandra connectors\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Log Analyst\") \\\n",
    "    .config(\"spark.jars.packages\",\n",
    "            \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.0,\"\n",
    "            \"com.datastax.spark:spark-cassandra-connector_2.12:3.0.0,\"\n",
    "            \"com.datastax.spark:spark-cassandra-connector-driver_2.12:3.0.0\") \\\n",
    "    .config(\"spark.cassandra.connection.port\", cassandra_port) \\\n",
    "    .config(\"spark.cassandra.connection.host\", cassandra_host) \\\n",
    "    .config(\"spark.cassandra.auth.username\", cassandra_user) \\\n",
    "    .config(\"spark.cassandra.auth.password\", cassandra_pwd) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1593e96d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Regular expression pattern for Common Log Format\n",
    "log_pattern = r'(\\S+) \\S+ \\S+ \\[([^\\]]+)\\] \"(\\S+) (\\S+) \\S+\" (\\d{3}) (\\S+)?'\n",
    "\n",
    "# Read streaming data from Kafka\n",
    "log_data = spark.readStream.format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_server) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load() \\\n",
    "    .selectExpr(\"CAST(value AS STRING) as value\") \\\n",
    "    # Extract fields from log line using regex\n",
    "    .withColumn(\"host\", regexp_extract(\"value\", log_pattern, 1)) \\\n",
    "    .withColumn(\"time\", regexp_extract(\"value\", log_pattern, 2)) \\\n",
    "    .withColumn(\"method\", regexp_extract(\"value\", log_pattern, 3)) \\\n",
    "    .withColumn(\"url\", regexp_extract(\"value\", log_pattern, 4)) \\\n",
    "    .withColumn(\"response\", regexp_extract(\"value\", log_pattern, 5)) \\\n",
    "    .withColumn(\"bytes\", regexp_extract(\"value\", log_pattern, 6)) \\\n",
    "    # Add processing timestamp\n",
    "    .withColumn(\"time_added\", unix_timestamp()) \\\n",
    "    # Extract file extension from URL\n",
    "    .withColumn(\"extension\",\n",
    "                when(split(col(\"url\"), \"\\.\").getItem(-1).isNull(), \"None\")\n",
    "                .otherwise(split(col(\"url\"), \"\\.\").getItem(-1))) \\\n",
    "    # Replace missing host with \"unknown\"\n",
    "    .withColumn(\"host\",\n",
    "                when(col(\"host\").isNull() | (col(\"host\") == \"\"), lit(\"unknown\"))\n",
    "                .otherwise(col(\"host\"))) \\\n",
    "    .drop(\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375b147d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to process each micro-batch\n",
    "def process_row(df, epoch_id):\n",
    "    # Write batch to Cassandra\n",
    "    df.write \\\n",
    "      .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "      .mode(\"append\") \\\n",
    "      .options(table=table_name, keyspace=key_space) \\\n",
    "      .save()\n",
    "    # Write batch to HDFS as CSV\n",
    "    df.coalesce(1).write.csv(\"hdfs://52.202.83.122:8020/output/nasa_logs/\", mode=\"append\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abffb99",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Start the streaming query\n",
    "query = log_data.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"checkpoint/data\") \\\n",
    "    .foreachBatch(process_row) \\\n",
    "    .start()\\\n",
    "    .awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
